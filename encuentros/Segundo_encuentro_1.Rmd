---
title: 'Web Scraping con R '
subtitle: 'Una introducci√≥n'
author:
- 'Agust√≠n Nieto - @agusnieto77'
- 'INHUS-CONICET-UNMDP'
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output:
  md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Web Scraping en R con el paquete rvest

## ¬øQu√© es el Web Scraping?

Se denomina 'web scraping' (en ingl√©s = ara√±ado o raspado web) a la extracci√≥n (automatizada y dirigida) y almacenamiento computacional del contenido de p√°ginas web. La informaci√≥n raspada puede ser de diverso tipo. Por ejemplo, contactos telef√≥nicos, correo electr√≥nico, direcciones f√≠sicas, informaci√≥n censal, notas period√≠sticas o de opini√≥n, comentarios de lectorxs, precios, etc. Esta informaci√≥n se almacena en formatos diversos: vectores l√≥gicos, num√©ricos o de texto plano, marcos de datos, tablas, listas, matrices, arrays. Los objetos de clase arrays son poco usuales. En este encuentro nos vamos a centrar en los objetos de tipo tabular (tibbles y data frames). Tambi√©n usaremos objetos de clase lista y vector.

![](https://estudiosmaritimossociales.org/Data_TalleR/tipos_objetos_r.png)

En t√©rminos generales, el web scraping toma informaci√≥n web semi-estructurada y la devuelve en un formato estructurado. Como dijimos, aqu√≠ usaremos el formato tibble.

## Web Scraping y el giro digital

En las √∫ltimas dos d√©cadas el crecimiento de la informaci√≥n online creci√≥ de forma acelerada, al punto de tornar imprescindible el uso del raspado web para la recuperaci√≥n masiva de parte de esa informaci√≥n nacida digital.
Internet alberga una cantidad infinita de datos "extraibles". Parte de esta informaci√≥n subyace en bases de datos, detr√°s de API o en texto plano enmarcados en estructuras HTML/XML. Como vimos en los encuentros anteriores, por distintas razones podemos querer obtener informaci√≥n de redes sociales como Twitter o de foros de usuarixs para ver qu√© est√° pensando la poblaci√≥n sobre distintos temas y t√≥picos. 
De todas formas, la accesibilidad no siempre est√° al alcance de la mano, muchas p√°ginas web bloquean el acceso mediante programaci√≥n o configuran "muros de pago" que requieren que se suscriba a una API para acceder. Esto es lo que hacen, por ejemplo, *The New York Times* y *El ABC*. Pero, finalmente, esas medidas no son una traba definitiva. Existen muchas oportunidades para obtener los datos que nos interesan.

## El Web Scraping y su legalidad

En t√©rminos generales, el raspado web (no comercial) de informaci√≥n publicada en la web y de acceso p√∫blico no es ilegal. Sin embargo, existen protocolos de buenas pr√°cticas de raspado que debemos intentar respetar por cuestiones √©ticas. Para m√°s detalles sobre este asunto pueden leer los siguientes art√≠culos: James Phoenix (2020) ['Is Web Scraping Legal?'](https://understandingdata.com/is-web-scraping-legal/), Tom Waterman (2020) ['Web scraping is now legal'](https://medium.com/@tjwaterman99/web-scraping-is-now-legal-6bf0e5730a78), Krotov, V., Johnson, L., & Silva, L. (2020) ['Tutorial: Legality and Ethics of Web Scraping'](https://aisel.aisnet.org/cgi/viewcontent.cgi?article=4240&context=cais), Edward Roberts (2018) ['Is Web Scraping Illegal? Depends on What the Meaning of the Word Is'](https://www.imperva.com/blog/is-web-scraping-illegal/).

## ¬øPara qu√© hacer Web Scraping?

Los usos del raspado web son infinitamente variados. Todo depende del problema que queramos resolver. Puede ser la recuperaci√≥n de la serie hist√≥rica de precios de los pasajes de autob√∫s en la ciudad de Mar del Plata. O el an√°lisis de las tendencias actuales en las agendas period√≠sticas en la prensa espa√±ola. Quiz√°s la detecci√≥n de cambios en el lenguaje a lo largo del tiempo referido al uso del lenguaje inclusivo, por ejemplo. O el monitoreo del humor social en determinado lugar y tiempo en torno a t√≥picos pol√≠ticos, sociales, culturales o econ√≥micos. Etc√©tera. Etc√©tera. Etc√©tera.  

Todo esto es independiente de la herramienta que usemos para hacer el raspado web. Pero no es as√≠ en este TalleR üòâ.

## ¬øC√≥mo hacer Web Scraping en R?

Esta pregunta la vamos a responder con un enfoque pr√°ctico, gracias a las funciones del paquete `rvest`. 

Lo primero que vamos a hacer es activar los paquetes que vamos a utilizar a lo largo de los ejercicios. El primero de los ejercicios nos permitir√° desarrollar una funci√≥n de web scraping. En este caso aplicaremos la funci√≥n creada a un diario espa√±ol: *El Mundo*. La funci√≥n nos permitir√° quedarnos con los titulares de una de sus secciones. Luego analizaremos esos titulares con t√©cnicas de tonkenizaci√≥n y, finalmente, visualizaremos los resultados con `ggplot2` que nos devolver√° un gr√°fico de barras con las palabras m√°s frecuentes. Esto nos permitir√° tener un primer pantallazo sobre la agenda period√≠stica del peri√≥dico en cuesti√≥n. Sin m√°s pre√°mbulo, pasemos la primer ejercicio.

### Ejercicio 1

¬øCu√°les son los t√≥picos m√°s importantes de la agenda del diario *El Mundo* seg√∫n las √∫ltimas notas de su secci√≥n 'Espa√±a'? Veamos:

```{r paquetes, warning=FALSE, message=FALSE, out.width='80%', fig.align='center'}
# Pueden copiar y pegar el script o descargarlo desde RStudio con esta l√≠nea de comando:
# utils::download.file("https://estudiosmaritimossociales.org/ejercicio01.R", "ejercicio01.R")


# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(tidyverse)
require(rvest)
require(stringr)
require(tidytext)


# Creamos la funci√≥n para raspar El Mundo cuyo nombre ser√° 'scraping_EM()' ------------------------

scraping_EM <- function (x){          # abro funci√≥n para raspado web y le asigno un nombre: scraping_EM
  
  rvest::read_html(x) %>%             # llamo a la funci√≥n read_html() para obtener el contenido de la p√°gina
    
    rvest::html_nodes(".ue-c-cover-content__headline-group") %>%  # llamo a la funci√≥n html_nodes() y especifico las etiquetas de los t√≠tulos 
    
    rvest::html_text() %>%            # llamo a la funci√≥n html_text() para especificar el formato 'chr' del t√≠tulo.
    
    tibble::as_tibble() %>%           # llamo a la funci√≥n as_tibble() para transforma el vector en tabla 
    
    dplyr::rename(titulo = value)     # llamo a la funci√≥n rename() para renombrar la variable 'value'
  
}                                     # cierro la funci√≥n para raspado web


# Usamos la funci√≥n para scrapear el diario El Mundo ----------------------------------------------

(El_Mundo <- scraping_EM("https://www.elmundo.es/espana.html"))


# Tokenizamos los t√≠tulos de la secci√≥n 'Espa√±a' del peri√≥dico El Mundo ---------------------------

El_Mundo %>%                                          # datos en formato tabular extra√≠dos con la funci√≥n scraping_EM()
  
  tidytext::unnest_tokens(                            # funci√≥n para tokenizar
    
    palabra,                                          # nombre de la columna a crear
    
    titulo) %>%                                       # columna de datos a tokenizar
  
  dplyr::count(                                       # funci√≥n para contar
    
    palabra) %>%                                      # columna de datos a contar
  
  dplyr::arrange(                                     # funci√≥n para ordenar columnas
    
    dplyr::desc(                                      # orden decreciente
      
      n)) %>%                                         # columna de frecuencia a ordenar en forma decreciente
  
  dplyr::filter(n > 4) %>%                            # filtramos y nos quedamos con las frecuencias mayores a 2
  
  dplyr::filter(!palabra %in% 
                  tm::stopwords("es")) %>%            # filtramos palabras comunes
  
  dplyr::filter(palabra != "espa√±a") %>%              # filtro comod√≠n
  
  dplyr::filter(palabra != "a√±os") %>%                # filtro comod√≠n
  
  ggplot2::ggplot(                                    # abrimos funci√≥n para visualizar
    
    ggplot2::aes(                                     # definimos el mapa est√©tico del gr√°fico
      
      y = n,                                          # definimos la entrada de datos de y
      
      x = stats::reorder(                             # definimos la entrada de datos de x
        
        palabra,                                      # con la funci√≥n reorder() 
        
        + n                                           # para ordenar de mayor a menos la frecuencia de palabras
        
      )                                               # cerramos la funci√≥n reorder()
      
    )                                                 # cerramos la funci√≥n aes()
    
  ) +                                                 # cerramos la funci√≥n ggplot()
  
  ggplot2::geom_bar(                                  # abrimos la funci√≥n geom_bar()
    
    ggplot2::aes(                                     # agregamos par√°metros a la funci√≥n aes()
      
      fill = as_factor(n)                             # definimos los colores y tratamos la variable n como factores
      
    ),                                                # cerramos la funci√≥n aes()
    
    stat = 'identity',                                # definimos que no tiene que contar, que los datos ya est√°n agrupados 
    
    show.legend = F) +                                # establecemos que se borre la leyenda
  
  ggplot2::geom_label(                                # definimos las etiquetas de las barras
    
    aes(                                              # agregamos par√°metros a la funci√≥n aes()
      
      label = n                                       # definimos los valores de ene como contenido de las etiquetas
      
    ),                                                # cerramos la funci√≥n aes()
    
    size = 5) +                                       # definimos el tama√±o de las etiquetas
  
  ggplot2::labs(                                      # definimos las etiquetas del gr√°fico
        
    title = "Temas en la agenda period√≠stica",        # definimos el t√≠tulo
    
    x = NULL,                                         # definimos la etiqueta de la x
    
    y = NULL                                          # definimos la etiqueta de la y
    
  ) +                                                 # cerramos la funci√≥n labs()
  
  ggplot2::coord_flip() +                             # definimos que las barras est√©n acostadas                     
  
  ggplot2::theme_bw() +                               # definimos el tema general del gr√°fico
  
  ggplot2::theme(                                     # definimos par√°metros para los ejes
    
    axis.text.x = 
      ggplot2::element_blank(),                       # definimos que el texto del eje x no se vea
    
    axis.text.y = 
      ggplot2::element_text(                          # definimos que el texto del eje y 
        
        size = 16                                     # definimos el tama√±o del texto del eje y
        
      ),                                              # cerramos la funci√≥n element_text()
    
    plot.title = 
      ggplot2::element_text(                          # definimos la est√©tica del t√≠tulo
        
        size = 18,                                    # definimos el tama√±o
        
        hjust = .5,                                   # definimos la alineaci√≥n 
        
        face = "bold",                                # definimos el grosor de la letra
        
        color = "darkred"                             # definimos el color de la letra
        
      )                                               # cerramos la funci√≥n element_text()
    
  )                                                   # cerramos la funci√≥n theme()

```

Parece que durante los √∫ltimos d√≠as los temas centrales fueron la covid, las pol√≠ticas publicas en torno al coronavirus (toque de queda, restricciones, confinamiento), disputas pol√≠ticas entre el gobierno y la oposici√≥n. 

### Ejercicio 2

Gracias al Ejercicio 1 tenemos una idea general sobre c√≥mo y para qu√© hacer web scraping. En el ejercicio 1 hicimos todo en uno, desde la extracci√≥n hasta la visualizaci√≥n. Ahora nos ocuparemos de ir paso a paso. Adem√°s, haremos un raspado un poco m√°s profundo. 

Arranquemos por la funci√≥n de web scraping:

```{r fun_scraping, warning=FALSE, message=FALSE}
# Pueden copiar y pegar o descargarlo desde RStudio con esta l√≠nea de comando:
# utils::download.file("https://estudiosmaritimossociales.org/ejercicio02.R", "ejercicio02.R")

# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(dplyr)
require(rvest)
require(tibble)


# Creamos la funci√≥n para raspar El Pa√≠s cuyo nombre ser√° 'scraping_links()' ---------------------

scraping_links <- function(pag_web, tag_link) {   # abro funci√≥n para raspado web y le asigno un nombre: scraping_links.
  
  rvest::read_html(pag_web) %>%                   # llamo a la funci√≥n read_html() para obtener el contenido de la p√°gina.
    
    rvest::html_nodes(tag_link) %>%               # llamo a la funci√≥n html_nodes() y especifico las etiquetas de los t√≠tulos 
    
    rvest::html_attr("href") %>%                  # llamo a la funci√≥n html_attr() para especificar el formato 'chr' del t√≠tulo.
    
    rvest::url_absolute(pag_web) %>%              # llamo a la funci√≥n url::absolute() para completar las URLs relativas
    
    tibble::as_tibble() %>%                       # llamo a la funci√≥n as_tibble() para transforma el vector en tabla
    
    dplyr::rename(link = value)                   # llamo a la funci√≥n rename() para renombrar la variable 'value'
  
}                                                 # cierro la funci√≥n para raspado web


# Usamos la funci√≥n para scrapear el diario El Mundo ----------------------------------------------

(links_EM <- scraping_links(pag_web = "https://www.elmundo.es/economia.html", tag_link = "a.ue-c-cover-content__link"))

# Usamos la funci√≥n para scrapear el diario El Pa√≠s -----------------------------------------------

(links_EP <- scraping_links(pag_web = "https://elpais.com/espana/", tag_link = "h2 a")) 

```

Cumplido el primer paso (la obtenci√≥n de los link a las notas completas), nos toca construir una funci√≥n para 'rascar' el contenido completo de cada nota. ¬°Manos a la obra!

```{r notas_scraping, warning=FALSE, message=FALSE}
# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(dplyr)
require(rvest)
require(tibble)


# Creamos la funci√≥n para raspar El Pa√≠s cuyo nombre ser√° 'scraping_links()' ---------------------

scraping_notas <- function(pag_web, tag_fecha, tag_titulo, tag_nota) { # abro funci√≥n para raspado web: scraping_notas().
  
  tibble::tibble(                               # llamo a la funci√≥n tibble.
  
  fecha = rvest::html_nodes(                    # declaro la variable fecha y llamo a la funci√≥n html_nodes().
    
    rvest::read_html(pag_web), tag_fecha) %>%   # llamo a la funci√≥n read_html(pag_web) y especifico la(s) etiqueta(s) de la fecha. 
    
    rvest::html_text(),                         # llamo a la funci√≥n html_text() para especificar el formato 'chr' de la fecha.
  
  titulo = rvest::html_nodes(                   # declaro la variable titulo y llamo a la funci√≥n html_nodes().
    
    rvest::read_html(pag_web), tag_titulo) %>%  # llamo a la funci√≥n read_html(pag_web) y especifico la(s) etiqueta(s) del titulo.  
    
    rvest::html_text(),                         # llamo a la funci√≥n html_text() para especificar el formato 'chr' del t√≠tulo.
  
  nota = rvest::html_nodes(                     # declaro la variable nota y llamo a la funci√≥n html_nodes(). 
    
    rvest::read_html(pag_web), tag_nota) %>%    # llamo a la funci√≥n html_nodes() y especifico la(s) etiqueta(s) de la nota. 
    
    rvest::html_text()                          # llamo a la funci√≥n html_text() para especificar el formato 'chr' del t√≠tulo.
  
  )                                             # cierro la funci√≥n tibble().
  
}                                               # cierro la funci√≥n para raspado web.


# Usamos la funci√≥n para scrapear las notas del diario El Pa√≠s u otras p√°ginas web ---------------------------

(notas_EP  <- scraping_notas(pag_web = "https://elpais.com/espana/2021-01-16/madrid-una-semana-enterrada-en-la-nieve.html", 
                             tag_fecha = ".a_ti",
                             tag_titulo = "h1",
                             tag_nota = ".a_b")) 

```

Result√≥ bien, pero ya tenemos un primer problema de normalizaci√≥n: el formato de la fecha. Cuando miramos el tibble vemos que la variable fecha es identificada y tratada como de tipo 'chr' (caracter). Debemos transformarla en una variable de tipo 'date' (fecha). ¬øC√≥mo lo hacemos? Hay muchas formas. Ac√° vamos a hacerlo en dos pasos. Primero vamos a quedarnos con los 11 caracteres iniciales ("dd mmm yyyy") y luego removemos los restantes. Finalmente, transformamos esos 11 caracteres en un valor 'date' con la funci√≥n `dmy()` del paquete `lubridate` de `tidyverse`. Veamos c√≥mo...

```{r lubridate, warning=FALSE, message=FALSE}
# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(lubridate)
require(stringr)
require(magrittr)


fecha_sin_normalizar <- "16 ene 2021 - 23:30 UTC"   # creamos el objeto 'fecha_sin_normalizar'.

(stringr::str_sub(fecha_sin_normalizar, 1, 11) %>%  # llamamos a la funci√≥n str_sub() para quedarnos con los primeros 11 caracteres.   
  
  stringr::str_replace_all("ene", "jan") %>%        # llamamos a la funci√≥n str_remplace_all() para cambiar la denominaci√≥n de los mes.             
  stringr::str_replace_all("abr", "apr") %>% 
  stringr::str_replace_all("ago", "aug") %>% 
  stringr::str_replace_all("dic", "dec") %>% 
  
  lubridate::dmy() -> fecha_normalizada)            # finalmente llamamos a la funci√≥n dmy() para transformar el string en un valor tipo 'date'.

base::class(fecha_normalizada)                      # examinamos su clase.

```

Bien. Hemos logrado transformar la cadena de caracteres que conten√≠a la fecha en un valor que R reconoce y trata como 'date'. Sin embargo, seguimos con un problema no menor. Pudimos recuperar con al funci√≥n scraping_notas() el contenido de una nota, pero la idea es recuperar el contenido de un set completo de notas. Para lograrlo tendremos que hacer uso de una nueva funci√≥n de la familia tidyverse que perteneciente al paquete `purrr`. Nos referimos a la funci√≥n `pmap_dfr()`. Esta funci√≥n es una variante de la funci√≥n `map()` de `purrr` que itera sobre m√∫ltiples argumentos simult√°neamente y en paralelo.

```{r set_notas, warning=FALSE, message=FALSE}

# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(dplyr)
require(rvest)
require(tibble)
require(purrr)


# Creamos la funci√≥n para raspar los links a las notas cuyo nombre ser√° 'scraping_links()' -------

scraping_links <- function(pag_web, tag_link) {   # abro funci√≥n para raspado web y le asigno un nombre: scraping_links
  
  rvest::read_html(pag_web) %>%                   # llamo a la funci√≥n read_html() para obtener el contenido de la p√°gina
    
    rvest::html_nodes(tag_link) %>%               # llamo a la funci√≥n html_nodes() y especifico las etiquetas de los t√≠tulos 
    
    rvest::html_attr("href") %>%                  # llamo a la funci√≥n html_attr() para especificar el formato 'chr' del t√≠tulo.
    
    rvest::url_absolute(pag_web) %>%              # llamo a la funci√≥n url::absolute() para completar las URLs relativas
    
    tibble::as_tibble() %>%                       # llamo a la funci√≥n as_tibble() para transforma el vector en tabla
    
    dplyr::rename(link = value)                   # llamo a la funci√≥n rename() para renombrar la variable 'value'
  
}                                                 # cierro la funci√≥n para raspado web


# Usamos la funci√≥n para scrapear los links a las notas de El Pa√≠s -------------------------------

(links_EP  <- scraping_links(pag_web = "https://elpais.com/espana/", tag_link = "h2 a")) 


# Creamos la funci√≥n para raspar El Pa√≠s cuyo nombre ser√° 'scraping_links()' ---------------------

scraping_notas <- function(pag_web, tag_fecha, tag_titulo, tag_nota) { # abro funci√≥n para raspado web: scraping_notas().
  
  tibble::tibble(                               # llamo a la funci√≥n tibble.
  
  fecha = rvest::html_nodes(                    # declaro la variable fecha y llamo a la funci√≥n html_nodes().
    
    rvest::read_html(pag_web), tag_fecha) %>%   # llamo a la funci√≥n read_html(pag_web) y especifico la(s) etiqueta(s) de la fecha. 
    
    rvest::html_text(),                         # llamo a la funci√≥n html_text() para especificar el formato 'chr' de la fecha.
  
  titulo = rvest::html_nodes(                   # declaro la variable titulo y llamo a la funci√≥n html_nodes().
    
    rvest::read_html(pag_web), tag_titulo) %>%  # llamo a la funci√≥n read_html(pag_web) y especifico la(s) etiqueta(s) del titulo.  
    
    rvest::html_text(),                         # llamo a la funci√≥n html_text() para especificar el formato 'chr' del t√≠tulo.
  
  nota = rvest::html_nodes(                     # declaro la variable nota y llamo a la funci√≥n html_nodes(). 
    
    rvest::read_html(pag_web), tag_nota) %>%    # llamo a la funci√≥n html_nodes() y especifico la(s) etiqueta(s) de la nota. 
    
    rvest::html_text()                          # llamo a la funci√≥n html_text() para especificar el formato 'chr' del t√≠tulo.
  
  )                                             # cierro la funci√≥n tibble().
  
}                                               # cierro la funci√≥n para raspado web.


# Seleccionamos los links que refieren a la secci√≥n que nos interesa y nos quedamos solo con 10 notas --------

(links_EP_limpio <- links_EP %>% filter(str_detect(link, "https://elpais.com/espana/")) %>% filter(!str_detect(link,"en-clave-de-bienestar")) %>% .[1:10,])


# Usamos la funci√≥n pmap_dfr() para emparejar los links y la funci√≥n de web scraping y creamos el objeto el_pais_esp con las 10 notas completas

(el_pais_esp <-                     # abrimos la funci√≥n print '(' y asignamos un nombre al objeto que vamos a crear.
    
    purrr::pmap_dfr(                # llamamos a la funci√≥n pmap_dfr() para emparejar links y funci√≥n de rascado.
      
      base::list(                   # Llamamos a la funci√≥n list() para crear una lista con los m√∫ltiples argumentos de la funci√≥n de rascado.
        
        links_EP_limpio$link,       # vector de links.
        
        ".a_ti",                    # etiqueta de fecha.
        
        "h1",                       # etiqueta de t√≠tulo.
        
        ".a_b"),                    # etiqueta de nota y cierro la funci√≥n list().
      
      scraping_notas))              # funci√≥n scraping_notas() sin los `()` y cierro la funci√≥n pmap_dfr() y la funci√≥n print `)`.


# Usamos la funci√≥n para scrapear los links a las notas de La Naci√≥n -------------------------------

(links_LN <- scraping_links(pag_web = "https://www.lanacion.com.ar/politica", tag_link = "h2 a"))


# Usamos la funci√≥n para scrapear las notas de La Naci√≥n. Replicamos todo en una sola l√≠nea de c√≥digo.

(la_nacion_ar <- purrr::pmap_dfr(list(links_LN$link[1:10],"section.fecha","h1.titulo","#cuerpo p"), scraping_notas))


# Guardamos el objeto 'la_nacion_ar' como objeto .rds

base::saveRDS(la_nacion_ar, "la_nacion_ar.rds")

```

Bueno, parece que finalmente completamos los pasos para hacer un web scraping completo. Pero esto no termina aqu√≠. Seguro notaron, por un lado, que las notas se trasformaron de 10 a m√°s de 200, y por otro lado, que la columna 'nota' del tibble la_nacion_ar est√° 'sucia', contiene datos no relevantes y que pueden ser contraproducente en el momento del an√°lisis. Tenemos que normalizar y limpiar esa columna (variable). ¬°Hag√°moslo!

```{r norm, warning=FALSE, message=FALSE}
# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(dplyr)
require(rvest)
require(tibble)
require(stringr)
require(tidyr)
require(lubridate)

# Cargamos el objeto la_nacion_ar.

la_nacion_ar <- base::readRDS("la_nacion_ar.rds")

# Imprimimos en consola sus valores completos, las notas completas.

la_nacion_ar$nota[1:10] # los corchetes me permiten seleccionar los valores seg√∫n su n√∫mero de fila

# Detectamos que hay algunas filas que son recurrente y debemos borrar:

   # "Celdas vac√≠as"

   # "Espacios en blanco"

   # "\r"

   # "\n"

   # "Conforme a los criterios de"

# Con el uso del paquete stringr vamos a remover estos fragmentos de informaci√≥n no √∫til.

(la_nacion_ar_limpia <- la_nacion_ar %>%                                  # creamos un nuevo objeto clase tibble.
    
    dplyr::mutate(nota = stringr::str_trim(nota)) %>%                     # con las funciones mutate() y str_trim() quitamos los espacios en blanco sobrantes.
    
    dplyr::filter(nota != "",                                             # con la funci√≥n filter() descartamos las celdas vac√≠as.

           nota != " ",                                                   # o que contiene solo un espacio en blanco.
           
           nota != "Conforme a los criterios de") %>%                     # tambi√©n descartamos valore recurrente que no forman parte de la nota.
    
    dplyr::mutate(nota_limpia = stringr::str_remove_all(nota, "\\\n"),    # con las funciones mutate() y str_remove_all() creamos la nueva variable nota_limpia, 
           
           nota_limpia = stringr::str_remove_all(nota_limpia, "\\\r")))   # removemos las \n = nueva l√≠nea (new line) y los \r = retorno de carro (return).


# Ahora colapsaremos los p√°rrafos de cada nota en una sola celda, de esta forma volveremos a un tibble de 10 filas (observaciones), una por nota.

(la_nacion_ar_limpia_norm <- la_nacion_ar_limpia %>%                                # creamos un nuevo objeto clase tibble.
    
  dplyr::group_by(fecha, titulo) %>%                                                # con la funci√≥n group_by() agrupamos por fecha y t√≠tulo.
    
  dplyr::summarise(nota_limpia = base::paste(nota_limpia, collapse = " ||| ")) %>%  # con las funciones summarise() y paste() colapsamos los p√°rrafos.
  
  dplyr::select(fecha, titulo, nota_limpia) %>%                                     # con la funci√≥n select() seleccionamos las variables. 
  
  dplyr::mutate(fecha = stringr::str_remove_all(fecha, "‚Ä¢.*$"),                     # con las funciones mutate() y str_remove_all() normalizamos el string de fechas.
           
                fecha = lubridate::dmy(fecha)))                                     # con las funciones mutate() y dmy() le damos formato date al string de fechas.

# Imprimimos en consola sus valores completos, las notas completas.

la_nacion_ar_limpia_norm$nota_limpia[1:10] # los corchetes me permiten seleccionar los valores seg√∫n su n√∫mero de fila

```

Hemos logrado lo que quer√≠amos, extraer informaci√≥n semi-estructurada de internet y transformar esa informaci√≥n en datos dentro de un marco de datos de tipo tabular (tabla). ¬°Bien hecho!

### Ejercicio 3

Ahora nos toca avanzar en otro de los enfoque para desarrollar web scraping. Cuando las p√°ginas no explicitan su url y necesitamos interactuar con el navegador s√≠ o s√≠, se vuelve necesario el auxilio del paquete `RSelenium`. 

![](https://estudiosmaritimossociales.org/Data_TalleR/la_nacion_selenium.png)

Este paquete, junto con `rvest`, nos permite scrapear p√°ginas din√°micas. Hay que tener en cuenta que este enfoque falla m√°s y es m√°s lento.

```{r rselenium, warning=FALSE, message=FALSE}
# Pueden copiar y pegar o descargarlo desde RStudio con esta l√≠nea de comando:
# utils::download.file("https://estudiosmaritimossociales.org/ejercicio03.R", "ejercicio03.R")

# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(tidyverse)
require(rvest)
# install.packages("RSelenium") (si no lo tienen instalado)
require(RSelenium) 

# El objetivo de RSelenium es facilitar la conexi√≥n a un servidor remoto desde dentro de R. 
# RSelenium proporciona enlaces R para el API de Selenium Webdriver. 
# Selenio es un proyecto centrado en la automatizaci√≥n de los navegadores web. 

# Descargamos los binarios, iniciamos el controlador y obtenemos el objeto cliente.

servidor <- RSelenium::rsDriver(browser = "firefox", port = 4542L) # iniciar un servidor y un navegador de Selenium

cliente <- servidor$client                                         # objeto 'cliente' (objeto que contiene un v√≠nculo din√°mico con el servidor)

cliente$navigate("https://www.lanacion.com.ar/politica")           # cargamos la p√°gina a navegar


# Ahora debemos encontrar el bot√≥n de carga y hacemos clic sobre √©l.

VerMas <- cliente$findElement(using = "xpath", "//*[(@id = 'verMas')] //a") # Encontramos el bot√≥n


for (i in 1:10){                # abrimos funci√≥n for() para reiterar n veces la acci√≥n (clic)
  
  base::print(i)                # imprimimos cada acci√≥n
  
  VerMas$clickElement()         # hacemos clic
  
  base::Sys.sleep(7)            # estimamos tiempo de espera entre clic y clic
  
}                               # cerramos la funci√≥n for()


html_data <- cliente$getPageSource()[[1]]                          # obtenemos datos HTML y los analizamos


ln_sec_pol <- html_data %>%                                        # obtenemos los links a las notas de la secci√≥n Pol√≠tica
  
  rvest::read_html() %>%                                           # leemos el objeto html_data con la funci√≥n read_html()
  
  rvest::html_nodes(".cuerpo h2 a") %>%                            # ubicamos los tags de los links a las notas
  
  rvest::html_attr("href") %>%                                     # extraemos los links de las notas
  
  rvest::url_absolute("https://www.lanacion.com.ar/politica") %>%  # llamo a la funci√≥n url::absolute() para completar las URLs relativas
  
  tibble::as_tibble() %>%                                          # llamo a la funci√≥n as_tibble() para transformar el objeto en una tibble.
  
  dplyr::rename(link = value)                                      # llamo a la funci√≥n rename() para renombrar la variable creada.


# Creamos la funci√≥n scraping_notas() para scrapear los links obtenidos ---------------------

scraping_notas <- function(pag_web, tag_fecha, tag_titulo, tag_nota) { # abro funci√≥n para raspado web: scraping_notas().
  
  tibble::tibble(                               # llamo a la funci√≥n tibble.
  
  fecha = rvest::html_nodes(                    # declaro la variable fecha y llamo a la funci√≥n html_nodes().
    
    rvest::read_html(pag_web), tag_fecha) %>%   # llamo a la funci√≥n read_html(pag_web) y especifico la(s) etiqueta(s) de la fecha. 
    
    rvest::html_text(),                         # llamo a la funci√≥n html_text() para especificar el formato 'chr' de la fecha.
  
  titulo = rvest::html_nodes(                   # declaro la variable `titulo` y llamo a la funci√≥n html_nodes().
    
    rvest::read_html(pag_web), tag_titulo) %>%  # llamo a la funci√≥n read_html(pag_web) y especifico la(s) etiqueta(s) del t√≠tulo.  
    
    rvest::html_text(),                         # llamo a la funci√≥n html_text() para especificar el formato 'chr' del t√≠tulo.
  
  nota = rvest::html_nodes(                     # declaro la variable nota y llamo a la funci√≥n html_nodes(). 
    
    rvest::read_html(pag_web), tag_nota) %>%    # llamo a la funci√≥n read_html(pag_web) y especifico la(s) etiqueta(s) de la nota.  
    
    rvest::html_text()                          # llamo a la funci√≥n html_text() para especificar el formato 'chr' de la nota.
  
  )                                             # cierro la funci√≥n tibble().
  
}                                               # cierro la funci√≥n para raspado web.


# Usamos la funci√≥n pmap_dfr() para emparejar los links y la funci√≥n de web scraping y 
# creamos el objeto la_nacion_politica con las 100 notas completas

(la_nacion_politica <- purrr::pmap_dfr(list(ln_sec_pol$link[1:100],"section.fecha","h1.titulo","#cuerpo p"), scraping_notas))


# Guardamos el objeto 'la_nacion_politica' como objeto .rds

base::saveRDS(la_nacion_politica, "la_nacion_politica.rds")

```

### Ejercicio 4

No todo es informaci√≥n suelta y poco estructurada. El lenguaje HTML tiene un objeto que presenta su contenido en formato tabular, nos referimos a las tablas HTML que tienen las etiquetas <table></table>. Es verdad que muchas de estas tablas tiene la opci√≥n de descarga en formato `csv` y otro similar, pero no siempre es as√≠. Inspeccionemos un poco.

En Wikipedia, un sitio hiper consultado, las tablas no tren por defecto la opci√≥n de descarga. A ver... 

![](https://estudiosmaritimossociales.org/Data_TalleR/wiki.png)

Ah√≠ est√°n los datos sobre poblaci√≥n mundial. Los queremos pero no los podemos bajar en ning√∫n formato. Podemos copiar y pegar o 'rasparlos' de forma autom√°tica...

```{r wiki, warning=FALSE, message=FALSE}
# Pueden copiar y pegar o descargarlo desde RStudio con esta l√≠nea de comando:
# utils::download.file("https://estudiosmaritimossociales.org/ejercicio04.R", "ejercicio04.R")

# Paquetes a cargar (funci√≥n 'require()' es equivalente a la funci√≥n 'library()') ----------------

require(dplyr)
require(rvest)
require(tibble)


# Creamos la funci√≥n para raspar El Pa√≠s cuyo nombre ser√° 'scraping_links()' ---------------------

url_wiki <- "https://es.wikipedia.org/wiki/Poblaci√≥n_mundial"  # creamos el objeto url_wiki con la url de la p√°g. web que contiene las tablas

(pob__mun__t_tablas <- rvest::read_html(url_wiki) %>%          # creamos un objeto y llamamos a la funci√≥n read_html() para leer la p√°g. web.
    
  rvest::html_table())                                         # llamamos a la funci√≥n html_table() para quedarnos con todas las tablas existentes.

(pob_mun_tablas_1y2 <- rvest::read_html(url_wiki) %>%          # creamos un objeto y llamamos a la funci√≥n read_html() para leer la p√°g. web.
    
  rvest::html_table() %>% .[1:2])                              # llamamos a la funci√≥n html_table() e indicamos con qu√© tablas queremos quedarnos.

(pob__mun__tabla__1 <- rvest::read_html(url_wiki) %>%          # creamos un objeto y llamamos a la funci√≥n read_html() para leer la p√°g. web.
    
  rvest::html_table() %>% .[[1]])                              # llamamos a la funci√≥n html_table() e indicamos con qu√© tabla queremos quedarnos.

(pob__mun__tabla__2 <- rvest::read_html(url_wiki) %>%          # creamos un objeto y llamamos a la funci√≥n read_html() para leer la p√°g. web.
    
  rvest::html_table() %>% .[[2]])                              # llamamos a la funci√≥n html_table() e indicamos con qu√© tabla queremos quedarnos.


saveRDS(pob_mun_tablas_1y2, 'pob_mun_tablas_1y2.rds')          # guardamos como archivo .rds la lista con los dos tibbles.

```

Pudimos bajar las dos tablas con datos referidos a la poblaci√≥n mundial. Con este ejercicio concluimos el cap√≠tulo sobre web scraping. 

## Bibliograf√≠a de referencia

* [Olgun Aydin (2018) *R web Scraping Quick Start Guide*](https://books.google.es/books?hl=es&lr=&id=Iel1DwAAQBAJ&oi=fnd&pg=PP1&dq=#v=onepage&q&f=false)
* [Alex Bradley & Richard J. E. James (2019) *Web Scraping Using R*](https://journals.sagepub.com/doi/pdf/10.1177/2515245919859535)
* [Mine Dogucu & Mine √áetinkaya-Rundel (2020) *Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities*](https://www.tandfonline.com/doi/pdf/10.1080/10691898.2020.1787116?needAccess=true)
* [Simon Munzert, Christian Rubba, Peter Mei√üner & Dominic Nyhuis (2015)	*Automated Data Collection with R: A Practical Guide to Web Scraping and Text Mining*](https://estudiosmaritimossociales.org/R_web_scraping.pdf)
* [Steve Pittard (2020) *Web Scraping with R*.](https://steviep42.github.io/webscraping/book/)
